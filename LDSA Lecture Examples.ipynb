{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Apache Spark development basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Word Count\n",
    "The word count program is considered to be the \"hello world\" program in Big Data analytics. In the word count program, given a text file, we want to count how many times every single word occurs. An example follows.\n",
    "\n",
    "**Input file:**\n",
    "```\n",
    "foo bar bar \n",
    "baz bar\n",
    "foo baz bar\n",
    "```\n",
    "**Result:** \n",
    "```\n",
    "(foo,2)\n",
    "(bar,4)\n",
    "(baz,2)\n",
    "```\n",
    "\n",
    "**Task 1:** given an input file `data/la_divina_commedia.txt`, count how many times each single word occurs into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "{\n",
    "sc.textFile(\"data/la_divina_commedia.txt\")\n",
    "  .flatMap(_.split(\" \"))\n",
    "  .map((_,1))\n",
    "  .reduceByKey(_+_)\n",
    "  .saveAsTextFile(\"data/commedia_counts.txt\")\n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Montecarlo $\\pi$ estimation\n",
    "Large dataset analysis is the main use case of Spark. However, Spark can be used to perform compute intensive tasks as well. Montecarlo $\\pi$ estimation is a good example problem.\n",
    "\n",
    "### Montecarlo method\n",
    "![circle](https://learntofish.files.wordpress.com/2010/10/circle_dots.png?w=300)\n",
    "\n",
    "**Idea:** the ratio $\\frac{A_{circle}}{A_{square}}$ is roughly equal to the faction of *darts* that fall in the circle.\n",
    "\n",
    "#### Algorithm\n",
    "1. Throw $N$ uniformly distributed darts in the square\n",
    "2. Count how many darts fall in the circle\n",
    "3. Pi is roughly $4\\frac{count}{N}$ \n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{count}{N} \\simeq \\frac{A_{circle}}{A_{square}} = \\frac{\\pi r^2}{(2r)^2} = \\frac{\\pi r^2}{4^2} = \\frac{\\pi}{4}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "3.141272\n"
     ]
    }
   ],
   "source": [
    "{\n",
    "val n = 10000000\n",
    "val count = sc.parallelize(1 to n)\n",
    "  .map { _ =>\n",
    "      val x = math.random\n",
    "      val y = math.random\n",
    "      if(x*x + y*y < 1) 1 else 0\n",
    "  }.reduce(_+_)\n",
    "val pi = 4.0 * count / n\n",
    "println(pi)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbour Classifier\n",
    "\n",
    "When Spark was first implemented, the motivation for having a new framework was the lack of dataset caching in MapReduce (and Hadoop). This is penalizing for applications that need to access a hot dataset iteratively. Building a K-Nearest Neighbour (KNN) classifier, is a nice example that falls in this range of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
