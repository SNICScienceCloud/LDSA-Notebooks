{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Spark Lab Assignment\n",
    "\n",
    "## General instructions \n",
    "The purpose of this assignment is to develop some basic Spark skills. The assignment is composed by two mandatory tasks and one challenge. Only PhD students are required to go through the challenge part. However, completing the challenges will improve the final grade for Master students too. \n",
    "\n",
    "**Warning:** all of the tasks **must** be solved using the Spark RDD API, in order to distribute the computations in the Spark workers.\n",
    "\n",
    "## How to get help for this assignment\n",
    "[GitHub](https://github.com/) is a web-based software development collaboration tool. It is very important for your career that you get familiar with it. This is why Q&A for this course will be based on GitHub issues. If you encounter any problem setting up the environment, or you need some additional pointers to solve the assignments please open an issue here: https://github.com/SNICScienceCloud/LDSA-Spark/issues.\n",
    "\n",
    "## Getting started\n",
    "\n",
    "### Setup the environment\n",
    "Before you begin with the assignment you need to get the Docker-based Spark deployment that we have seen in the lecture. You can either install the environment on the SNIC cloud or on your local computer. For Linux and Mac users, installing the environment locally should be straightforward, while on Windows machines the installation could be slightly more challenging. If you are unsure on how to proceed, the best way is to deploy the envirnment on SNIC.  \n",
    "\n",
    "**Setup the environment on the SNIC cloud (recommended)**\n",
    "\n",
    "1. Log into https://hpc2n.cloud.snic.se\n",
    "2. Start an instance from *TheSparkBox* image:\n",
    "\n",
    "    * Type an instance name\n",
    "    * Use the `scc.large` flavor\n",
    "    * Select the internal IPv4 network\n",
    "    * Add the `Spark` security group\n",
    "    * Select your Key Pair\n",
    "    * Launch the instance\n",
    " \n",
    "3. Associate a floating IP to the newly create instance, and SSH into it:\n",
    "    \n",
    "   ```\n",
    "   ssh core@<your-floating-ip>\n",
    "   ```\n",
    "\n",
    "4. Deploy Spark and Jupyter running:\n",
    "\n",
    "   ```\n",
    "   export SPARK_PUBLIC_DNS=\"<your-floating-ip>\"\n",
    "   export TSB_JUPYTER_TOKEN=\"<your-password>\"\n",
    "   tsb up -d\n",
    "   ```\n",
    "\n",
    "If everithing went well, within a couple of minutes you should be able to access the web UIs.\n",
    "\n",
    "   * **Spark UI**: `http://<your-floating-ip>:8080`\n",
    "   * **Jupyter**: `http://<your-floating-ip>:8888`\n",
    "\n",
    "\n",
    "**Setup the environment locally**\n",
    "\n",
    "If you like to work on you local computer, and you can install [Docker](https://www.docker.com/), here you can find detailed instruction to get the environment on your machine: https://github.com/mcapuccini/TheSparkBox. The procedure should be straightforward for Linux and Mac, however on Windows this could a little bit more challenging.\n",
    "\n",
    "### Import the course material\n",
    "The material for the Spark module in this course is stored in this GitHub repository: https://github.com/SNICScienceCloud/LDSA-Spark. You can import it in your deployment following this steps:\n",
    "\n",
    "1. Log into *Jupyter*\n",
    "2. Start a new Jupyter terminal: `New > Terminal`\n",
    "3. Clone the course material running: \n",
    "   \n",
    "   ```\n",
    "   git clone https://github.com/SNICScienceCloud/LDSA-Spark.git\n",
    "   ```\n",
    "\n",
    "If everithing went well, you should be able to open this assignment as a Jupyter notebook. Please go back to the Jupyter home page and navigate to `LDSA-Spark > Lab Assignment.ipynb`. The `LDSA-Spark` folder includes also the `Lecture Examples.ipynb` notebook that we have seen in the lecture, so you can play with it. However, keep in mind that only one notebook at the time will manage to send tasks to the Spark Master, so make sure to shutdown notebooks when you are not using them. \n",
    "\n",
    "### How to submit the assignment\n",
    "Please complete the tasks in this assignment editing this notebook, both for the code implementation and the theory questions. If you are not familiar with Jupyter, you may want to give a look to the [Notebook Basics tutorial](http://nbviewer.jupyter.org/github/jupyter/notebook/blob/master/docs/source/examples/Notebook/Notebook%20Basics.ipynb). When you are done with the tasks, please save this notebook with your answers, download it as `.ipynb` (`File > Download as > Notebook`), and upload it to the student portal.  \n",
    "\n",
    "## Task 1: DNA G+C precentage\n",
    "DNA is a molecule that carries genetic information used in the growth, development, functioning and reproduction of all known living organisms. The DNA information is coded in a language of 4 bases: cytosine (C), guanine (G), adenine (A), thymine (T). The percentage of G+C bases in a DNA sequence has valuable biological mening (wikipedia: https://goo.gl/kCLvDp), hence it is important to be able to compute it for long DNA sequences.\n",
    "\n",
    "### Task\n",
    "Given an input DNA sequence, represented as a text file: `data/dna.txt`, compute the percentage of `g` + `c` occurrences into it. An example follows:\n",
    "\n",
    "**Input file:**\n",
    "```\n",
    "atcg\n",
    "ccgg\n",
    "ttat\n",
    "```\n",
    "**result:** \n",
    "$$\\frac{C_{count} + G_{count}}{C_{count} + G_{count} + A_{count} + T_{count}} = \\frac{6}{12} = 0.5$$\n",
    "\n",
    "**Tip 1:** when you load an input file as an RDD, each line will be loaded into a distinct string RDD record. In Scala you can count the occurrences of a certain character in a string as it follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"atttccgg\".count(c => c == 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question 1:** Is the previous operation parallel, or is computed locally? Why?\n",
    "\n",
    "*Answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Tip 2:** sums form different RDD records can be aggregated using the RDD *reduce* method. An example follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sumsRDD = sc.parallelize(Array(3,5,2))\n",
    "sumsRDD.reduce(_+_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "// Implementation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question 2:** What is an RDD in Spark?\n",
    "\n",
    "*Answer goes here*\n",
    "\n",
    "**Question 3:** Is *reduce* a trasformation or an action? What are RDD transformations and RDD actions? How do they differ from each other?\n",
    "\n",
    "*Answer goes here*\n",
    "\n",
    "**Question 4:** What are some of the advanteges of Spark over Hadoop (and MapReduce)?\n",
    "\n",
    "*Answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task 2: Monte Carlo integration\n",
    "Large dataset analysis is the main use case of Spark. However, Spark can be used to perform compute intensive tasks as well. Numerical integration is a good example problem that falls in this group of use cases. \n",
    "\n",
    "<img src=\"img/montecarlo_int.png\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The **Monte Carlo integration** method, is a way to get an approximation of the definite integral of a function $f(x)$, over an interval $[A,B]$. Given a value $Max_{f(x)},$ which $f(x)$ never exceeds, we first randomly draw $N$ uniformly distributed points $(x_1,y_2) … (x_N,y_N)$ s.t. $x_1 … x_N \\in [A,B], y_1 … y_N \\in [0,Max_{f(x)}]$. Then, assuming that $f(x)$ is positive over $[A,B]$, the fraction of points that fell under $f(x)$ will be roughly equal to the area under the curve, divided by the total area of the rectangle in which we randomly drew points. Hence, the definite integral of $f(x)$ over $[A,B]$ is roughly equal to: \n",
    "\n",
    "$$(B-A) Max_{f(x)}\\frac{n_{P}}{tot_{P}},$$\n",
    "where $n_{P}$ is the number of points that fell under $f(x)$, and $tot_{P}$ is the total number of randomly drawn points.\n",
    "\n",
    "### Task \n",
    "Write a program in Spark to approximate the definite integral of $f(x) = (1 + sin(x))$ / $cos(x)$ over $[0,1].$ Such function is positive and it is lower than $4$ over $[0,1]$. \n",
    "\n",
    "![integral](img/integral.gif)\n",
    "\n",
    "For the purpose of this assignment drawing 1000 points is good enough.\n",
    "\n",
    "**Question** What does the Spark's parallelize function do? What is it good for?\n",
    "\n",
    "*Answer goes here*\n",
    "\n",
    "### Your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "// Implementation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Challenge: Iris flower classification\n",
    "\n",
    "The well-known **Iris flower dataset** (wiki: https://goo.gl/OQjope) contains measurements for 150 *Iris* flowers form 3 different species: *Iris setosa*, *Iris virginica* and *Iris versicolor*. For each example in the dataset, 4 measurements has been performed: *sepal length*, *sepal width*, *petal length* and *petal width*.\n",
    "\n",
    "![iris](img/iris.jpeg)\n",
    "\n",
    "You are given a copy of this dataset in Comma Separated Values (CSV) format: `data/iris_csv.txt`. In the CSV text file each line contains *sepal length*, *sepal width*, *petal length*, *petal width* and *species name* separated by a comma. The file looks something like the following example:\n",
    "\n",
    "```\n",
    "5.1,3.5,1.4,0.2,Iris-setosa\n",
    "4.9,3.0,1.4,0.2,Iris-setosa\n",
    "4.7,3.2,1.3,0.2,Iris-setosa\n",
    "7.0,3.2,4.7,1.4,Iris-versicolor\n",
    "6.4,3.2,4.5,1.5,Iris-versicolor\n",
    "6.9,3.1,4.9,1.5,Iris-versicolor\n",
    "6.7,3.3,5.7,2.5,Iris-virginica\n",
    "6.7,3.0,5.2,2.3,Iris-virginica\n",
    "6.3,2.5,5.0,1.9,Iris-virginica\n",
    "```\n",
    "\n",
    "**Task 1:** Build and evaluate a 3NN classifier for the **Iris flower dataset** in Spark. In order to evaluate your classifier, you can save 20% of the data for testing like we did in the lecture examples. For simplicity, you are allowed to collect the test data.\n",
    "\n",
    "**N.B.** Part of the challenge is to figure out how to parse the input dataset into an RDD. Google is your friend!\n",
    "\n",
    "### Your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "// Implementation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Task 2:** If you think that competition is fun, you can try out different values of *K* for the KNN classifier, different distance measures, or any kind of strategy that you think it will improve the results. Submit your best calssifier implementation, and we'll make a ranking that will be available to all of the students that feel like taking part of this competition.\n",
    "\n",
    "**Warning:** in order to keep the evaluation unbiased, you are not allowed to use the class information in the test set to make predictions.\n",
    "\n",
    "### Your best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "// Your best classifier, don't forget to include evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
