{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Apache Spark development basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Check that Spark is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Word Count\n",
    "The word count program is considered to be the \"hello world\" program in Big Data analytics. In the word count program, given a text file, we want to count how many times every single word occurs. An example follows.\n",
    "\n",
    "**Input file:**\n",
    "```\n",
    "foo bar bar \n",
    "baz bar\n",
    "foo baz bar\n",
    "```\n",
    "**Result:** \n",
    "```\n",
    "(foo,2)\n",
    "(bar,4)\n",
    "(baz,2)\n",
    "```\n",
    "\n",
    "**Task 1:** given an input file `data/la_divina_commedia.txt`, count how many times each single word occurs into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "{\n",
    "sc.textFile(\"data/la_divina_commedia.txt\")\n",
    "  .flatMap(_.split(\" \")) // transformation\n",
    "  .map((_,1)) // transformation\n",
    "  .reduceByKey(_+_) // transformation\n",
    "  .saveAsTextFile(\"data/commedia_counts.txt\") // action (triggers the computation)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Montecarlo $\\pi$ estimation\n",
    "Large dataset analysis is the main use case of Spark. However, Spark can be used to perform compute intensive tasks as well. Montecarlo $\\pi$ estimation is a good example problem.\n",
    "\n",
    "### Montecarlo method\n",
    "![circle](img/montecarlo_pi.png)\n",
    "\n",
    "**Idea:** the ratio $\\frac{A_{circle}}{A_{square}}$ is roughly equal to the faction of *darts* that fall in the circle.\n",
    "\n",
    "#### Algorithm\n",
    "1. Throw $N$ uniformly distributed darts in the square\n",
    "2. Count how many darts fall in the circle\n",
    "3. Pi is roughly $4\\frac{count}{N}$ \n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{count}{N} \\simeq \\frac{A_{circle}}{A_{square}} = \\frac{\\pi r^2}{(2r)^2} = \\frac{\\pi r^2}{4^2} = \\frac{\\pi}{4}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "3.141292\n"
     ]
    }
   ],
   "source": [
    "{\n",
    "val n = 10000000\n",
    "val count = sc.parallelize(1 to n)\n",
    "  .map { _ =>\n",
    "      val x = math.random\n",
    "      val y = math.random\n",
    "      if(x*x + y*y < 1) 1 else 0\n",
    "  }.reduce(_+_)\n",
    "val pi = 4.0 * count / n\n",
    "println(pi)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## K-Nearest Neighbour Classifier\n",
    "\n",
    "When Spark was first implemented, the motivation to have a new framework was the lack of dataset caching in MapReduce (and Hadoop). This is penalizing for applications that need to access a hot dataset iteratively. Building a K-Nearest Neighbour (KNN) classifier, is a nice example that falls in this range of problems.\n",
    "\n",
    "**Supervised classification:** Given a dataset of examples $(x_i, l)$, $i=1 ... N$, where $x_n$ is a vector in a fixed dimensional space and $l$ is a class label, in supervised classification we aim to build a model to classify new unlabelled exaples $(x_i, ?)$, $i=N+1 ... M$.\n",
    "\n",
    "**KNN, idea:** Foreach new example $(x_i, ?)$, $i=N+1 ... M$, compute the (euclidean) distance from the known examples $(x_i, l)$, $i=1 ... N$. Label $(x_i, ?)$ as the most frequent class in the K nearest neighbours. \n",
    "\n",
    "<img src=\"img/knn.png\" width=\"60%\"/>\n",
    "\n",
    "### 1-NN classifer in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of correct predictions 0.9295774647887324\n"
     ]
    }
   ],
   "source": [
    "{\n",
    "\n",
    "// Generate some examples \n",
    "val examples = sc.parallelize(1 to 300).map { _ =>\n",
    "    val xi = Array(math.random, math.random)\n",
    "    val label = if(xi(1) > xi(0)) {\n",
    "        \"yellow\"\n",
    "    } else {\n",
    "        \"purple\"\n",
    "    }\n",
    "    (xi,label)\n",
    "}\n",
    "\n",
    "// Save 20% of the exaples for testing\n",
    "val split = examples.randomSplit(Array(0.8,0.2))\n",
    "val dataset = split(0).cache // cache in memory\n",
    "val testset = split(1).collect // assume M small\n",
    "\n",
    "// Make some prediction with 1NN\n",
    "val predictions = testset.map { case(x,_) => // unseen\n",
    "    dataset.map { case(z, label) => // known label\n",
    "        val d0 = z(0) - x(0)\n",
    "        val d1 = z(1) - x(1)\n",
    "        val dist = math.sqrt(d0*d0 + d1*d1)\n",
    "        (dist,label)\n",
    "    }\n",
    "    .sortBy{case(dist,label) => dist} \n",
    "    .first\n",
    "    ._2 // return the label\n",
    "}\n",
    "\n",
    "// Evaluate our predictions\n",
    "val correct = testset.zip(predictions).count { \n",
    "    case((_,label),prediction) =>\n",
    "        label == prediction\n",
    "}\n",
    "val correctFrac = correct.toDouble / testset.length\n",
    "println(s\"fraction of correct predictions $correctFrac\")\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Questions\n",
    "Please open an issue here: https://goo.gl/dOy089."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
